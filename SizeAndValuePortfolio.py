# -*- coding: utf-8 -*-
"""PS4_406306460_Sol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBXR8LsnRlBmZ16RuPukz0IP7ds71bHf
"""

import warnings
warnings.filterwarnings('ignore')

!pip install wrds

!pip install --upgrade numpy scipy

"""# Problem Set 4: Size and Value

## Loading Data and other Pre-requisites
"""

import pandas as pd
import numpy as np
import os
import datetime
import time
import gc
import matplotlib.pyplot as plt
import wrds
import pandas_datareader
from pandas.tseries.offsets import *

# connecting with the wrds account
conn = wrds.Connection(wrds_username="vrai")

########################################################################################################################
## Compustat (data from wrds)
########################################################################################################################
cstat = conn.raw_sql("""
                    select a.gvkey, a.datadate, a.at, a.pstkl, a.txditc, a.fyear, a.ceq, a.lt,
                    a.mib, a.itcb, a.txdb, a.pstkrv, a.seq, a.pstk, b.sic, b.year1, b.naics
                    from comp.funda as a
                    left join comp.names as b
                    on a.gvkey = b.gvkey
                    where indfmt='INDL'
                    and datafmt='STD'
                    and popsrc='D'
                    and consol='C'
                    """)
cstat.head()

# Pension data
Pension = conn.raw_sql("""
                        select gvkey, datadate, prba
                        from comp.aco_pnfnda
                        where indfmt='INDL'
                        and datafmt='STD'
                        and popsrc='D'
                        and consol='C'
                        """)
Pension.head()

########################################################################################################################
## CRSP-Compustat link table (data from wrds)
########################################################################################################################
crsp_cstat = conn.raw_sql("""
                  select gvkey, lpermno as permno, lpermco as permco, linktype, linkprim, liid,
                  linkdt, linkenddt
                  from crspq.ccmxpf_linktable
                  where substr(linktype,1,1)='L'
                  and (linkprim ='C' or linkprim='P')
                  """)
crsp_cstat.head()

########################################################################################################################
## CRSP returns (data from wrds)
########################################################################################################################
crsp_raw = conn.raw_sql("""
                      select a.permno, a.permco, a.date, b.exchcd, b.siccd, b.naics,
                      a.ret, a.retx, a.shrout, a.prc
                      from crspq.msf as a
                      left join crspq.msenames as b
                      on a.permno=b.permno
                      and b.namedt<=a.date
                      and a.date<=b.nameendt
                      where b.shrcd in (10,11)
                      and b.exchcd in (1,2,3)
                      """)
crsp_raw = crsp_raw.sort_values(['permno', 'date']).reset_index(drop=True).copy()
crsp_raw['permno'] = crsp_raw['permno'].astype(int)
crsp_raw['permco'] = crsp_raw['permco'].astype(int)
crsp_raw['date'] = pd.to_datetime(crsp_raw['date'], format='%Y-%m-%d', errors='ignore') + MonthEnd(0)

crsp_raw.head()

# Load CRSP Delisting returns
dlret_raw = conn.raw_sql("""
                        select a.permno, a.permco, a.dlret, a.dlretx, a.dlstdt,
                        b.exchcd as dlexchcd, b.siccd as dlsiccd, b.naics as dlnaics
                        from crspq.msedelist as a
                        left join crspq.msenames as b
                        on a.permno=b.permno
                        and b.namedt<=a.dlstdt
                        and a.dlstdt<=b.nameendt
                        where b.shrcd in (10,11)
                        and b.exchcd in (1,2,3)
                        """)

dlret_raw = dlret_raw.sort_values(['permno', 'dlstdt']).reset_index(drop=True).copy()
dlret_raw.permno = dlret_raw.permno.astype(int)
dlret_raw['dlstdt'] = pd.to_datetime(dlret_raw['dlstdt'])
dlret_raw['date'] = dlret_raw['dlstdt'] + MonthEnd(0)
conn.close()

dlret_raw.head()

# merging the delisted returns with the original returns in crsp data
crsp_dlret_merged = pd.merge(crsp_raw, dlret_raw,
                             on=['permno', 'date'],
                             how='left')
crsp_dlret_merged['dlret'].fillna(0, inplace=True)
crsp_dlret_merged.rename(columns={'permco_x': 'permco'}, inplace=True)
# Drop the column 'permco_y'
crsp_dlret_merged.drop(columns=['permco_y', 'dlexchcd', 'dlsiccd', 'dlnaics', 'dlstdt'], inplace=True)
del dlret_raw
del crsp_raw
gc.collect()
crsp_dlret_merged.head()

########################################################################################################################
## Merging CRSP with Linktable
########################################################################################################################

# Step 1: merge all links
crsp_dlret_merged = crsp_dlret_merged.sort_values(['permco','permno','date']).reset_index(drop=True).copy()
crsp_linktable = crsp_dlret_merged.merge(crsp_cstat, on=['permno','permco'], how='inner')
numdup = np.sum(crsp_linktable[['permno', 'date']].duplicated(keep='first'))
print('Step 1: Number of Link Duplicates after merging: ' + str(numdup))

# Step 2: restrict to valid links
crsp_linktable = crsp_linktable[(crsp_linktable['linkdt'].isna() | (crsp_linktable['date'] >= crsp_linktable['linkdt'])) & (crsp_linktable['linkenddt'].isna() | (crsp_linktable['date'] <= crsp_linktable['linkenddt']))].copy()
crsp_linktable = crsp_linktable.sort_values(by=['permno', 'date']).reset_index(drop=True).copy()
numdup = np.sum(crsp_linktable[['permno', 'date']].duplicated(keep='first'))
print('After Step 2 (valid links): Number of Link Duplicates: ' + str(numdup))

# Step 3: if LC not LC linktype, only keep LC (LC stands for "Link research complete. Standard connection between databases.")
crsp_linktable['keep'] = True
crsp_linktable['flag'] = np.where(crsp_linktable['linktype'] == 'LC', 1, 0)  # Link research complete. Standard connection between databases.
crsp_linktable = crsp_linktable.merge(crsp_linktable[['permno', 'date', 'flag']].groupby(['permno', 'date']).sum().reset_index().rename(columns={'flag': 'ct_flag'}),on=['permno', 'date'], how='left')
crsp_linktable.loc[crsp_linktable[['permno', 'date']].duplicated(keep=False) & (crsp_linktable['ct_flag'] >= 1) & (crsp_linktable['flag'] == 0), 'keep'] = False
crsp_linktable = crsp_linktable[crsp_linktable['keep']].copy()
crsp_linktable.drop(['keep', 'flag', 'ct_flag'], axis=1, inplace=True)
numdup = np.sum(crsp_linktable[['permno', 'date']].duplicated(keep='first'))
print('After Step 3 (keep linktype=LC): Number of Link Duplicates: ' + str(numdup))

# Step 4: if P and not P linkprim, only keep P
crsp_linktable['keep'] = True
crsp_linktable['flag'] = np.where((crsp_linktable['linkprim'] == 'P'), 1, 0)  # Primary Link Marker: "P" indicates a primary link marker, as identified by Compustat in monthly security data.
crsp_linktable = crsp_linktable.merge(crsp_linktable[['permno', 'date', 'flag']].groupby(['permno', 'date']).sum().reset_index().rename(columns={'flag': 'ct_flag'}),on=['permno', 'date'], how='left')
crsp_linktable.loc[crsp_linktable[['permno', 'date']].duplicated(keep=False) & (crsp_linktable['ct_flag'] >= 1) & (crsp_linktable['flag'] == 0), 'keep'] = False
crsp_linktable = crsp_linktable[crsp_linktable['keep']].copy()
crsp_linktable.drop(['keep', 'flag', 'ct_flag'], axis=1, inplace=True)
numdup = np.sum(crsp_linktable[['permno', 'date']].duplicated(keep='first'))
print('After Step 4 (keep keeep linkprim=P): Number of Link Duplicates: ' + str(numdup))

# Dropping linktable variable that are no longer needed
crsp_linktable.drop(axis=1,columns=['linktype','linkprim','liid','linkdt','linkenddt'], inplace=True)
crsp_linktable.head()

# Fama and French 3 Factors
# pd.set_option('precision', 2)
data2 = pandas_datareader.famafrench.FamaFrenchReader('F-F_Research_Data_Factors',start='1900', end=str(datetime.datetime.now().year+1))
french = data2.read()[0] / 100 # Monthly data
french['Mkt'] = french['Mkt-RF'] + french['RF']

# Book-to-Market Portfolios
data2 = pandas_datareader.famafrench.FamaFrenchReader('Portfolios_Formed_on_BE-ME',start='1900', end=str(datetime.datetime.now().year+1))
data2 = data2.read()[0][['Lo 10', 'Dec 2', 'Dec 3', 'Dec 4', 'Dec 5', 'Dec 6', 'Dec 7', 'Dec 8', 'Dec 9', 'Hi 10']] / 100
data2.columns = 'BM01','BM02','BM03','BM04','BM05','BM06','BM07','BM08','BM09','BM10'
french = pd.merge(french,data2,how='left',on=['Date'])

# Size Portfolios
data2 = pandas_datareader.famafrench.FamaFrenchReader('Portfolios_Formed_on_ME',start='1900', end=str(datetime.datetime.now().year+1))
data2 = data2.read()[0][['Lo 10', 'Dec 2', 'Dec 3', 'Dec 4', 'Dec 5', 'Dec 6', 'Dec 7', 'Dec 8', 'Dec 9', 'Hi 10']] / 100
data2.columns = 'ME01','ME02','ME03','ME04','ME05','ME06','ME07','ME08','ME09','ME10'
french = pd.merge(french,data2,how='left',on=['Date'])

# 25 Book-to-Market and Size Portfolios
data2 = pandas_datareader.famafrench.FamaFrenchReader('25_Portfolios_5x5',start='1900', end=str(datetime.datetime.now().year+1))
data2 = data2.read()[0].rename(columns={"SMALL LoBM":"ME1 BM1","SMALL HiBM":"ME1 BM5","BIG LoBM":"ME5 BM1","BIG HiBM":"ME5 BM5"}) / 100
french = pd.merge(french,data2,how='left',on=['Date'])

# Changing date format and save
french = french.reset_index().rename(columns={"Date":"date"})
french['date'] = pd.DataFrame(french[['date']].values.astype('datetime64[ns]')) + MonthEnd(0)
print('Data from Ken French Website:')
print(french.columns)
french.head()

"""## Problem 1: Combining CRSP and Compustat to define size and book-to-market decile portfolios"""

# Compute returns including delisting returns
aux = crsp_linktable[(crsp_linktable['ret'].isna() & crsp_linktable['dlret'].isna())].index
crsp_linktable['dlret']   = crsp_linktable['dlret'].fillna(0)
crsp_linktable['ret']     = crsp_linktable['ret'].fillna(0)
crsp_linktable['ret']    = (1 + crsp_linktable['ret']) * (1 + crsp_linktable['dlret']) - 1
crsp_linktable.loc[aux,['ret']] = np.nan

# Market Equity (Market Capitalization)
crsp_linktable['me_total'] = crsp_linktable['prc'].abs() * crsp_linktable['shrout']  # calculate market equity

# Lagged Market Equity (to be used as weights)
crsp_linktable = crsp_linktable.sort_values(by=['permno','date']).reset_index().drop('index',axis=1).copy()
crsp_linktable['daten'] = crsp_linktable['date'].dt.year*12 + crsp_linktable['date'].dt.month
crsp_linktable['IsValidLag'] = crsp_linktable['daten'].diff(1) == 1 # Lag date has to be the lagged date
crsp_linktable.loc[crsp_linktable[crsp_linktable['permno'].diff(1) != 0].index,['IsValidLag']] = False # Lagged date has to be the same security
crsp_linktable['Lme'] = crsp_linktable[['permno','me_total']].groupby('permno').shift(1)
crsp_linktable.loc[crsp_linktable[crsp_linktable['IsValidLag'] == False].index,['Lme']] = np.nan
crsp = crsp_linktable.drop(['IsValidLag','daten'], axis=1)

# Substituing missing returns with zero (no trade assumption)
crsp.loc[crsp['ret'].isna(),'ret'] = 0

# Dropping missing Lagged market cap data (zero weight in the market portfolio)
crsp = crsp.loc[crsp['Lme'] > 0,:].copy()

# Sorting by date and permno
crsp = crsp.sort_values(by=['date','permno']).reset_index().drop('index',axis=1).copy()
del crsp_linktable
del crsp_dlret_merged
gc.collect()

crsp.head()

cstat_merged = cstat.merge(Pension, on = ['gvkey', 'datadate'], how = 'outer')
cstat_merged.head()

# Define Shareholders' equity (SHE)
cstat_merged['SHE'] = np.where(cstat_merged['seq'].notna(), cstat_merged['seq'],
                     np.where(cstat_merged['ceq'].notna() & cstat_merged['pstk'].notna(), cstat_merged['ceq'] + cstat_merged['pstk'],
                     np.where(cstat_merged['at'].notna() & cstat_merged['lt'].notna() & cstat_merged['mib'].notna(), cstat_merged['at'] - cstat_merged['lt'] - cstat_merged['mib'],
                     np.where(cstat_merged['at'].notna() & cstat_merged['lt'].notna(), cstat_merged['at'] - cstat_merged['lt'], np.nan))))

# Define Deferred taxes and investment tax credit (DT)
cstat_merged['DT'] = np.where(cstat_merged['txditc'].notna(), cstat_merged['txditc'],
                    np.where(cstat_merged['itcb'].notna() & cstat_merged['txdb'].notna(), cstat_merged['itcb'] + cstat_merged['txdb'],
                    np.where(cstat_merged['itcb'].notna(), cstat_merged['itcb'],
                    np.where(cstat_merged['txdb'].notna(), cstat_merged['txdb'], 0))))

# Define Book value of preferred stock (PS)
cstat_merged['PS'] = np.where(cstat_merged['pstkrv'].notna(), cstat_merged['pstkrv'],
                    np.where(cstat_merged['pstkl'].notna(), cstat_merged['pstkl'],
                    np.where(cstat_merged['pstk'].notna(), cstat_merged['pstk'], 0)))


# Handle prba: if NaN, replace with 0
cstat_merged['prba'] = cstat_merged['prba'].fillna(0)

# Define book equity (BE)
cstat_merged['BE'] = cstat_merged['SHE'] - cstat_merged['PS'] + cstat_merged['DT'] - cstat_merged['prba']

# Keep only positive book equity (BE)
cstat_merged['BE'] = np.where(cstat_merged['BE'] > 0, cstat_merged['BE'], np.nan)

# Display the first few rows of the updated DataFrame
cstat_merged.head()

crsp = crsp[crsp['date'].dt.year >= 1972]
crsp['year'] = crsp['date'].dt.year
crsp['month'] = crsp['date'].dt.month

# Filter Compustat data for dates from 1973 onwards
cstat_merged['datadate'] = pd.to_datetime(cstat_merged['datadate'])
cstat_merged = cstat_merged[cstat_merged['datadate'].dt.year >= 1972]

# Extract year from datadate in Compustat data
cstat_merged['year'] = cstat_merged['datadate'].dt.year

combined_data = pd.merge(crsp, cstat_merged, how='outer', on=['gvkey', 'year'], suffixes=('_crsp', '_compustat'))

del cstat_merged
del crsp
gc.collect()
combined_data.head()

columns_to_drop = ['siccd', 'naics_crsp', 'retx', 'dlret', 'dlretx', 'at', 'naics_compustat', 'SHE', 'DT',
                   'PS', 'pstkl', 'txditc', 'fyear', 'ceq', 'lt', 'mib', 'itcb', 'txdb', 'pstkrv', 'seq', 'pstk', 'sic',
                    'year1', 'prba']

# Drop the specified columns
combined_data = combined_data.drop(columns=columns_to_drop)
print(combined_data.columns)

# Filter for December data to get December market equity
december_me = combined_data[combined_data['month'] == 12][['gvkey', 'year', 'me_total']].copy()

# Rename columns for clarity
december_me = december_me.rename(columns={'year': 'dec_year', 'me_total': 'dec_me'})

# Create a lag year column to merge with the main dataset
december_me['lag_year'] = december_me['dec_year'] + 1

# Merge lagged December market equity to the main dataset for the next year
combined_data = pd.merge(combined_data, december_me[['gvkey', 'lag_year', 'dec_me']],
                         left_on=['gvkey', 'year'], right_on=['gvkey', 'lag_year'], how='left')

# Now, the `dec_me` column represents the lagged market equity for December of the previous year
combined_data['lag_dec_me'] = combined_data['dec_me']

# Ensure `lag_dec_me` is forward-filled to retain data for months 1-5
combined_data['lag_dec_me'] = combined_data.groupby('gvkey')['lag_dec_me'].fillna(method='ffill')

# Now filter for June data to calculate June market equity
combined_data['june_me'] = np.where(combined_data['month'] == 6, combined_data['me_total'], np.nan)
combined_data['june_me'] = combined_data.groupby('gvkey')['june_me'].fillna(method='ffill')

# Calculate lagged book equity for December of t-1
combined_data['lag_be'] = combined_data.groupby('gvkey')['BE'].shift(1)

# Calculate book-to-market ratio
combined_data['bm'] = combined_data['lag_be'] * 1000 / combined_data['lag_dec_me']

# Filter out rows without the required data for analysis purposes
combined_data = combined_data.dropna(subset=['lag_dec_me', 'june_me', 'lag_be'])

# Ensure the date columns are set properly
combined_data.reset_index(drop=True, inplace=True)
combined_data['year'] = combined_data['date'].dt.year
combined_data['month'] = combined_data['date'].dt.month

combined_data.head()

# Define brkpts_flag for NYSE
combined_data['brkpts_flag'] = combined_data['exchcd'] == 1

# factor bins
def yearly_bins(df, k, s, method):
    def diff_brkpts(x, k, method):
        brkpts_flag = x['brkpts_flag']
        signal = x[s]
        loc_nyse = signal.notna() & brkpts_flag
        nyse_signal = x.iloc[np.where(loc_nyse)[0]]

        if method in ["bm", "size"]:
            percentiles = [round(1/k * i, 2) for i in range(1, k)]
            breakpoints = nyse_signal[s].describe(percentiles=percentiles)
            breakpoints = breakpoints.loc['min':'max'].values.tolist()
        elif method == "smb":
            breakpoints = nyse_signal[s].describe(percentiles=[0.5])
            breakpoints = breakpoints.loc['min':'max'].values.tolist()
        elif method == "hml":
            breakpoints = nyse_signal[s].describe(percentiles=[0.3, 0.7])
            breakpoints = breakpoints.loc[['min', "30%", "70%", 'max']].values.tolist()

        y = pd.cut(x[s], bins=breakpoints, labels=False, include_lowest=True) + 1
        return y

    # Define the name mapping for different methods
    name_mapping = {
        'size': 'Size_Port',
        'bm': 'BtM_Port',
        'smb': 'SMB_Port',
        'hml': 'HML_Port'
    }

    bins = df.groupby('year').apply(lambda x: diff_brkpts(x, k, method)).reset_index(level=0, drop=True)
    bins = bins.to_frame()
    bins.columns = [name_mapping.get(method, f'{s}_bin')]

    return bins


# Creating size deciles
size_deciles = yearly_bins(combined_data, k=10, s='june_me', method='size')
combined_data = combined_data.reset_index(drop=True).merge(size_deciles, left_index=True, right_index=True, how='left')

# Creating book-to-market deciles
bm_deciles = yearly_bins(combined_data, k=10, s='bm', method='bm')
combined_data = combined_data.merge(bm_deciles, left_index=True, right_index=True, how='left')

# Creating HML bins (3 bins for high, medium, low book-to-market)
hml_bins = yearly_bins(combined_data, k=3, s='bm', method='hml')
combined_data = combined_data.merge(hml_bins, left_index=True, right_index=True, how='left')

# Creating SMB bins (2 bins for small, big size)
smb_bins = yearly_bins(combined_data, k=2, s='june_me', method='smb')
combined_data = combined_data.merge(smb_bins, left_index=True, right_index=True, how='left')

# Ensure the date columns are set properly
combined_data['year'] = combined_data['date'].dt.year
combined_data['month'] = combined_data['date'].dt.month

# Display the head of the combined data
combined_data.head()

from scipy.stats import skew

# Define the calculate_value_weighted_returns function
def calculate_value_weighted_returns(df, decile_col):
    df['weighted_ret'] = df['ret'] * df['Lme']
    monthly_returns = df.groupby(['date', decile_col]).apply(lambda x: x['weighted_ret'].sum() / x['Lme'].sum()).reset_index()
    monthly_returns.columns = ['date', decile_col, 'vw_ret']
    return monthly_returns

# Calculate monthly value-weighted returns for size deciles
size_vw_returns = calculate_value_weighted_returns(combined_data, 'Size_Port')

# Calculate monthly value-weighted returns for book-to-market deciles
bm_vw_returns = calculate_value_weighted_returns(combined_data, 'BtM_Port')

# Ensure the date columns are in datetime format
size_vw_returns['date'] = pd.to_datetime(size_vw_returns['date'])
bm_vw_returns['date'] = pd.to_datetime(bm_vw_returns['date'])

# Merge the risk-free rate from the 'french' dataset
size_vw_returns = size_vw_returns.merge(french[['date', 'RF']], on='date', how='left')
bm_vw_returns = bm_vw_returns.merge(french[['date', 'RF']], on='date', how='left')

# Calculate excess returns for size and book-to-market deciles
size_vw_returns['excess_ret'] = size_vw_returns['vw_ret'] - size_vw_returns['RF']
bm_vw_returns['excess_ret'] = bm_vw_returns['vw_ret'] - bm_vw_returns['RF']

size_vw_returns = size_vw_returns[(size_vw_returns['date'].dt.year >= 1973) & (size_vw_returns['date'].dt.year <= 2023)]
bm_vw_returns = bm_vw_returns[(bm_vw_returns['date'].dt.year >= 1973) & (bm_vw_returns['date'].dt.year <= 2023)]

# Calculate long-short returns for size (1-10)
long_short_size_returns = size_vw_returns[size_vw_returns['Size_Port'] == 1].set_index('date')['vw_ret'] - \
                          size_vw_returns[size_vw_returns['Size_Port'] == 10].set_index('date')['vw_ret']
long_short_size_returns = long_short_size_returns.reset_index().rename(columns={'vw_ret': 'long_short_ret'})
long_short_size_returns = long_short_size_returns.merge(french[['date', 'RF']], on='date', how='left')
long_short_size_returns['excess_ret'] = long_short_size_returns['long_short_ret'] - long_short_size_returns['RF']

# Calculate long-short returns for book-to-market (10-1)
long_short_bm_returns = bm_vw_returns[bm_vw_returns['BtM_Port'] == 10].set_index('date')['vw_ret'] - \
                        bm_vw_returns[bm_vw_returns['BtM_Port'] == 1].set_index('date')['vw_ret']
long_short_bm_returns = long_short_bm_returns.reset_index().rename(columns={'vw_ret': 'long_short_ret'})
long_short_bm_returns = long_short_bm_returns.merge(french[['date', 'RF']], on='date', how='left')
long_short_bm_returns['excess_ret'] = long_short_bm_returns['long_short_ret'] - long_short_bm_returns['RF']

# Define the calculate_annualized_statistics function
def calculate_annualized_statistics(monthly_returns, decile_col=None, return_col='excess_ret'):
    statistics = []

    if decile_col:
        for decile in monthly_returns[decile_col].unique():
            decile_data = monthly_returns[monthly_returns[decile_col] == decile]

            # Monthly statistics
            mean_return = decile_data[return_col].mean()
            volatility = decile_data[return_col].std()
            sharpe_ratio = mean_return / volatility
            skewness = skew(decile_data[return_col])

            # Annualized statistics
            annualized_mean_return = mean_return * 12
            annualized_volatility = volatility * np.sqrt(12)
            annualized_sharpe_ratio = annualized_mean_return / annualized_volatility

            statistics.append({
                'decile': decile,
                'annualized_mean_return': annualized_mean_return,
                'annualized_volatility': annualized_volatility,
                'annualized_sharpe_ratio': annualized_sharpe_ratio,
                'skewness': skewness
            })
    else:
        # Monthly statistics
        mean_return = monthly_returns[return_col].mean()
        volatility = monthly_returns[return_col].std()
        sharpe_ratio = mean_return / volatility
        skewness = skew(monthly_returns[return_col])

        # Annualized statistics
        annualized_mean_return = mean_return * 12
        annualized_volatility = volatility * np.sqrt(12)
        annualized_sharpe_ratio = annualized_mean_return / annualized_volatility

        statistics.append({
            'decile': 'long-short',
            'annualized_mean_return': annualized_mean_return,
            'annualized_volatility': annualized_volatility,
            'annualized_sharpe_ratio': annualized_sharpe_ratio,
            'skewness': skewness
        })

    return pd.DataFrame(statistics)

# Calculate annualized statistics for size deciles
size_annualized_stats = calculate_annualized_statistics(size_vw_returns, 'Size_Port')

# Calculate annualized statistics for book-to-market deciles
bm_annualized_stats = calculate_annualized_statistics(bm_vw_returns, 'BtM_Port')

# Calculate annualized statistics for long-short portfolios
long_short_size_annualized_stats = calculate_annualized_statistics(long_short_size_returns)
long_short_bm_annualized_stats = calculate_annualized_statistics(long_short_bm_returns)

# Combine results into a single DataFrame
size_annualized_stats['portfolio'] = 'size'
bm_annualized_stats['portfolio'] = 'bm'
long_short_size_annualized_stats['portfolio'] = 'size'
long_short_bm_annualized_stats['portfolio'] = 'bm'

annualized_stats = pd.concat([size_annualized_stats, bm_annualized_stats, long_short_size_annualized_stats, long_short_bm_annualized_stats], ignore_index=True)

size_annualized_stats00

"""## Problem 2: Annualized average excess returns, volatility, Sharpe Ratio, and skewness and comparison with French's portfolios: Size Decile"""

# annualized_stats = annualized_stats.sort_values(by='decile').reset_index(drop=True)
(annualized_stats[annualized_stats['portfolio'] == 'size'])

# Filter the french DataFrame for dates after 1973
french_filtered = french[french['date'].dt.year > 1973]

# Select the relevant columns for Book-to-Market and Size portfolios
bm_columns = ['BM01', 'BM02', 'BM03', 'BM04', 'BM05', 'BM06', 'BM07', 'BM08', 'BM09', 'BM10']
size_columns = ['ME01', 'ME02', 'ME03', 'ME04', 'ME05', 'ME06', 'ME07', 'ME08', 'ME09', 'ME10']

# Extract the relevant columns for decile returns from your dataset
size_decile_returns = size_vw_returns.pivot(index='date', columns='Size_Port', values='vw_ret')
size_decile_returns.columns = [f'Size_Port_{int(col)}' for col in size_decile_returns.columns]

bm_decile_returns = bm_vw_returns.pivot(index='date', columns='BtM_Port', values='vw_ret')
bm_decile_returns.columns = [f'BtM_Port_{int(col)}' for col in bm_decile_returns.columns]

# Merge the French decile returns with your decile returns
merged_size_returns = french_filtered[['date'] + size_columns].merge(size_decile_returns, on='date', how='inner')
merged_bm_returns = french_filtered[['date'] + bm_columns].merge(bm_decile_returns, on='date', how='inner')

# Calculate correlations for each decile
size_correlations = {f'ME{i:02d}': merged_size_returns[f'ME{i:02d}'].corr(merged_size_returns[f'Size_Port_{i}']) for i in range(1, 11)}
bm_correlations = {f'BM{i:02d}': merged_bm_returns[f'BM{i:02d}'].corr(merged_bm_returns[f'BtM_Port_{i}']) for i in range(1, 11)}

# Convert to DataFrame for easier viewing
size_correlations_df = pd.DataFrame.from_dict(size_correlations, orient='index', columns=['Correlation'])
bm_correlations_df = pd.DataFrame.from_dict(bm_correlations, orient='index', columns=['Correlation'])

size_correlations_df

# Calculate correlations for long-short returns
merged_size_long_short = french_filtered[['date', 'ME01', 'ME10']].merge(long_short_size_returns, on='date', how='inner')
merged_bm_long_short = french_filtered[['date', 'BM01', 'BM10']].merge(long_short_bm_returns, on='date', how='inner')

# Calculate the long-short returns for Fama-French
merged_size_long_short['french_long_short'] = merged_size_long_short['ME01'] - merged_size_long_short['ME10']
merged_bm_long_short['french_long_short'] = merged_bm_long_short['BM10'] - merged_bm_long_short['BM01']

# Calculate correlations for long-short returns
size_long_short_correlation = merged_size_long_short['french_long_short'].corr(merged_size_long_short['long_short_ret'])
bm_long_short_correlation = merged_bm_long_short['french_long_short'].corr(merged_bm_long_short['long_short_ret'])

# Display the correlations
print(f'Long-Short Size Correlation: {size_long_short_correlation}')
print(f'Long-Short Book-to-Market Correlation: {bm_long_short_correlation}')

"""## Problem 3: Annualized average excess returns, volatility, Sharpe Ratio, and skewness and comparison with French's portfolios: Book-to-Market Decile"""

(annualized_stats[annualized_stats['portfolio'] == 'bm'])

bm_correlations_df

"""## Problem 4: Performance of Value and Size Anomalies"""

# Plot the returns series
plt.figure(figsize=(20, 6))
plt.plot(long_short_size_returns['date'], long_short_size_returns['excess_ret'], label='Long-Short Size Returns')
plt.plot(long_short_bm_returns['date'], long_short_bm_returns['excess_ret'], label='Long-Short BTM Returns')
plt.plot(french_filtered['date'], french_filtered['Mkt-RF'], label='Market-RF Returns')
plt.xlabel('Date')
plt.ylabel('Excess Returns')
plt.title('Excess Returns Series')
plt.legend()
plt.show()

# Calculate cumulative returns
long_short_size_returns['cumulative_ret'] = (1 + long_short_size_returns['excess_ret']).cumprod()
long_short_bm_returns['cumulative_ret'] = (1 + long_short_bm_returns['excess_ret']).cumprod()
french_filtered['cumulative_ret'] = (1 + french_filtered['Mkt-RF']).cumprod()

# Plot cumulative returns
plt.figure(figsize=(10, 6))
plt.plot(long_short_bm_returns['date'], long_short_bm_returns['cumulative_ret'], label='Long-Short Value Cumulative Returns')
plt.plot(french_filtered['date'], french_filtered['cumulative_ret'], label='Excess Market Cumulative Returns')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.title('Cumulative Returns Series')
plt.legend()
plt.show()

# Plot cumulative returns with dual y-axes
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot long-short size cumulative returns on the first y-axis
ax1.plot(long_short_size_returns['date'], long_short_size_returns['cumulative_ret'], color='tab:blue', label='Long-Short Size Cumulative Returns')
ax1.set_xlabel('Date')
ax1.set_ylabel('Long-Short Size Cumulative Returns', color='tab:blue')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# Create a second y-axis for the Market-RF cumulative returns
ax2 = ax1.twinx()
ax2.plot(french_filtered['date'], french_filtered['cumulative_ret'], color='tab:orange', label='Market-RF Cumulative Returns')
ax2.set_ylabel('Excess Market Cumulative Returns', color='tab:orange')
ax2.tick_params(axis='y', labelcolor='tab:orange')

# Add title and legend
fig.suptitle('Cumulative Returns Series')
fig.tight_layout()
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9), bbox_transform=ax1.transAxes)

plt.show()

# Create a table with the Annualized Mean Returns, Volatility, and Sharpe Ratio for the long-short portfolios and Market-RF
def calculate_annualized_stats(series):
    mean_return = series.mean()
    volatility = series.std()
    sharpe_ratio = mean_return / volatility
    annualized_mean_return = mean_return * 12
    annualized_volatility = volatility * np.sqrt(12)
    annualized_sharpe_ratio = annualized_mean_return / annualized_volatility
    return annualized_mean_return, annualized_volatility, annualized_sharpe_ratio

long_short_size_stats = calculate_annualized_stats(long_short_size_returns['excess_ret'])
long_short_bm_stats = calculate_annualized_stats(long_short_bm_returns['excess_ret'])
market_rf_stats = calculate_annualized_stats(french['Mkt-RF'])

stats_table = pd.DataFrame({
    'Annualized Mean Return': [long_short_size_stats[0], long_short_bm_stats[0], market_rf_stats[0]],
    'Annualized Volatility': [long_short_size_stats[1], long_short_bm_stats[1], market_rf_stats[1]],
    'Annualized Sharpe Ratio': [long_short_size_stats[2], long_short_bm_stats[2], market_rf_stats[2]]
}, index=['Long-Short Size', 'Long-Short BTM', 'Market-RF'])

stats_table

"""## Problem 5: Annualized average excess returns, volatility, Sharpe Ratio, and skewness and comparison with French's portfolios: HML and SMB Portfolio"""

# Calculate HML and SMB factors
strategy = combined_data.groupby(['year', 'month', 'SMB_Port', 'HML_Port']).apply(lambda x: (x['ret'] * x['Lme']).sum() / x['Lme'].sum()).to_frame().reset_index().rename(columns={0: 'vwret'})

# Calculate HML and SMB factors
strategy = combined_data.groupby(['date', 'SMB_Port', 'HML_Port']).apply(lambda x: (x['ret'] * x['Lme']).sum() / x['Lme'].sum()).to_frame().reset_index().rename(columns={0: 'vwret'})

# Pivot the strategy DataFrame to compute HML and SMB
strategy_pivot = strategy.pivot_table(index='date', columns=['SMB_Port', 'HML_Port'], values='vwret')
strategy_pivot.columns = [f'{int(size)}_{int(bm)}' for size, bm in strategy_pivot.columns]

strategy_pivot['HML_Ret'] = (strategy_pivot['1_3'] + strategy_pivot['2_3']) / 2 - (strategy_pivot['1_1'] + strategy_pivot['2_1']) / 2
strategy_pivot['SMB_Ret'] = (strategy_pivot['1_1'] + strategy_pivot['1_2'] + strategy_pivot['1_3']) / 3 - (strategy_pivot['2_1'] + strategy_pivot['2_2'] + strategy_pivot['2_3']) / 3

# Merge the risk-free rate from the 'french' dataset
strategy_pivot.reset_index(inplace=True)
strategy_pivot = strategy_pivot.merge(french[['date', 'RF']], on='date', how='left')

# Calculate excess returns for HML and SMB
strategy_pivot['HML_Excess_Ret'] = strategy_pivot['HML_Ret'] - strategy_pivot['RF']
strategy_pivot['SMB_Excess_Ret'] = strategy_pivot['SMB_Ret'] - strategy_pivot['RF']

strategy_pivot = strategy_pivot[(strategy_pivot['date'].dt.year >= 1973) & (strategy_pivot['date'].dt.year <= 2023)]

# Calculate annualized statistics for HML and SMB
def calculate_hml_smb_annualized_statistics(monthly_returns, return_col):
    mean_return = monthly_returns[return_col].mean()
    volatility = monthly_returns[return_col].std()
    sharpe_ratio = mean_return / volatility
    skewness = skew(monthly_returns[return_col])

    annualized_mean_return = mean_return * 12
    annualized_volatility = volatility * np.sqrt(12)
    annualized_sharpe_ratio = annualized_mean_return / annualized_volatility

    statistics = {
        'annualized_mean_return': annualized_mean_return,
        'annualized_volatility': annualized_volatility,
        'annualized_sharpe_ratio': annualized_sharpe_ratio,
        'skewness': skewness
    }

    return statistics

hml_stats = calculate_hml_smb_annualized_statistics(strategy_pivot, 'HML_Excess_Ret')
smb_stats = calculate_hml_smb_annualized_statistics(strategy_pivot, 'SMB_Excess_Ret')

hml_annualized_stats = pd.DataFrame([hml_stats], index=['HML'])
smb_annualized_stats = pd.DataFrame([smb_stats], index=['SMB'])

# Combine all results
annualized_stats = pd.concat([size_annualized_stats, bm_annualized_stats], ignore_index=True)
annualized_stats = pd.concat([annualized_stats, hml_annualized_stats, smb_annualized_stats], ignore_index=True)

hml_annualized_stats

smb_annualized_stats

# Filter the french DataFrame for dates between 1973 and 2023
french_filtered = french[(french['date'].dt.year >= 1973) & (french['date'].dt.year <= 2023)]

# Merge the French HML and SMB returns with your calculated HML and SMB returns
merged_hml_returns = french_filtered[['date', 'HML']].merge(strategy_pivot[['date', 'HML_Ret']], on='date', how='inner')
merged_smb_returns = french_filtered[['date', 'SMB']].merge(strategy_pivot[['date', 'SMB_Ret']], on='date', how='inner')

# Calculate correlations for HML and SMB
hml_correlation = merged_hml_returns['HML'].corr(merged_hml_returns['HML_Ret'])
smb_correlation = merged_smb_returns['SMB'].corr(merged_smb_returns['SMB_Ret'])

# Display the correlations
print(f'HML Correlation: {hml_correlation}')
print(f'SMB Correlation: {smb_correlation}')

merged_hml_returns

# Calculate cumulative returns for HML and SMB from your data
merged_hml_returns['HML_Ret_Cumulative'] = (1 + merged_hml_returns['HML_Ret']).cumprod()
merged_smb_returns['SMB_Ret_Cumulative'] = (1 + merged_smb_returns['SMB_Ret']).cumprod()

# Plot cumulative returns for HML strategy with dual y-axes
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot calculated HML cumulative returns on the first y-axis
ax1.plot(merged_hml_returns['date'], merged_hml_returns['HML_Ret_Cumulative'], color='tab:blue', label='HML Cumulative Returns')
ax1.set_xlabel('Date')
ax1.set_ylabel('HML Cumulative Returns', color='tab:blue')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# Create a second y-axis for the calculated SMB cumulative returns
ax2 = ax1.twinx()
ax2.plot(merged_smb_returns['date'], merged_smb_returns['SMB_Ret_Cumulative'], color='tab:orange', label='SMB Cumulative Returns')
ax2.set_ylabel('SMB Cumulative Returns', color='tab:orange')
ax2.tick_params(axis='y', labelcolor='tab:orange')

# Add title and legend
fig.suptitle('Cumulative Returns Series - HML and SMB Strategies')
fig.tight_layout()
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9), bbox_transform=ax1.transAxes)

plt.show()

"""## Problem 6: Characteristic versus Factor Portfolios

The answer is added to a separate PDF as a write-up.
"""